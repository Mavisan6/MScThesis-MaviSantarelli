{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab65c40c-ad48-4909-8306-2ca373dcaf37",
   "metadata": {},
   "source": [
    "# Model Comparison and Ensemble Modelling\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Data Preparation](#2.-Data-Preparation)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### **Objective:**\n",
    "To assess and compare the performance of multiple SDMs (GLM, GAM, RF, XGBoost, MaxEnt) and explore ensemble modelling techniques to improve species distribution predictions.\n",
    "\n",
    "### **Rationale:**\n",
    "Comparing different models allows us to identify strengths and weaknesses in their predictive capabilities. Ensemble modelling combines multiple models to leverage their individual strengths, often leading to improved accuracy and robustness in predictions (Meller et al., 2014). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89103cbf-791f-4b2f-ae1b-81790fcbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, \n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674897e0-d571-47da-84ef-a73de1f9346f",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### **Steps:**\n",
    "\n",
    "#### 1. Load Test Predictions:\n",
    "- Import the test prediction results from each model for all species.\n",
    "\n",
    "#### 2. Ensure Consistency:\n",
    "- Verify that all datasets have consistent formatting, with aligned columns for true labels and predicted probabilities.\n",
    "\n",
    "### **Rationale:**\n",
    "Consistent data formatting is crucial for accurate performance evaluation and comparison across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0574fe6f-e9ac-4aa9-8c80-021c50717c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All models, test predictions, and metrics loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory where all models & results are stored\n",
    "base_dir = r\"C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\"\n",
    "\n",
    "# Species list\n",
    "species_list = [\"Bufo bufo\", \"Rana temporaria\", \"Lissotriton helveticus\"]\n",
    "\n",
    "# Dictionary to store all loaded data\n",
    "models = {\"GLM\": {}, \"GAM\": {}, \"RF\": {}, \"XGBoost\": {}, \"MaxEnt\": {}}\n",
    "metrics = {\"GLM\": {}, \"GAM\": {}, \"RF\": {}, \"XGBoost\": {}, \"MaxEnt\": {}}\n",
    "test_predictions = {\"GLM\": {}, \"GAM\": {}, \"RF\": {}, \"XGBoost\": {}, \"MaxEnt\": {}}\n",
    "\n",
    "# =========================== 1️⃣ Load GLM Models & Predictions ===========================\n",
    "glm_dir = os.path.join(base_dir, \"Final_GLM\")\n",
    "glm_pred_dir = os.path.join(glm_dir, \"GLM_Predictions\")\n",
    "glm_versions = [\"Lasso\", \"Ridge\"]\n",
    "\n",
    "for species in species_list:\n",
    "    models[\"GLM\"][species] = {}\n",
    "    metrics[\"GLM\"][species] = {}\n",
    "    test_predictions[\"GLM\"][species] = {}  # ✅ Initialise as an empty dictionary\n",
    "\n",
    "    for glm_type in glm_versions:\n",
    "        # Load GLM models\n",
    "        model_path = os.path.join(glm_dir, \"Models\", f\"{species}_GLM_{glm_type}_Threshold_0.3_Model.pkl\")\n",
    "        if os.path.exists(model_path):\n",
    "            models[\"GLM\"][species][glm_type] = joblib.load(model_path)\n",
    "        \n",
    "        # Load GLM test predictions\n",
    "        pred_path = os.path.join(glm_pred_dir, f\"{species}_GLM_{glm_type}_TestPredictions.csv\")\n",
    "        if os.path.exists(pred_path):\n",
    "            test_predictions[\"GLM\"][species][glm_type] = pd.read_csv(pred_path)  # ✅ Now it won't throw KeyError\n",
    "        \n",
    "        # Load GLM metrics\n",
    "        metrics_path = os.path.join(glm_dir, f\"{species}_Threshold_0.3_Metrics.txt\")\n",
    "        if os.path.exists(metrics_path):\n",
    "            with open(metrics_path, \"r\") as f:\n",
    "                metrics[\"GLM\"][species][glm_type] = f.read()\n",
    "\n",
    "# =========================== 2️⃣ Load GAM Models & Predictions ===========================\n",
    "gam_dir = os.path.join(base_dir, \"Final_GAM\")\n",
    "gam_pred_dir = os.path.join(gam_dir, \"GAM_Predictions\")\n",
    "\n",
    "for species in species_list:\n",
    "    test_predictions[\"GAM\"][species] = {}  # ✅ Initialise dictionary\n",
    "\n",
    "    # Load GAM model\n",
    "    model_path = os.path.join(gam_dir, f\"{species}_GAM_Model_CV.pkl\")\n",
    "    if os.path.exists(model_path):\n",
    "        models[\"GAM\"][species] = joblib.load(model_path)\n",
    "\n",
    "    # Load GAM test predictions\n",
    "    pred_path = os.path.join(gam_pred_dir, f\"{species}_GAM_TestPredictions.csv\")\n",
    "    if os.path.exists(pred_path):\n",
    "        test_predictions[\"GAM\"][species] = pd.read_csv(pred_path)\n",
    "\n",
    "    # Load GAM metrics\n",
    "    metrics_path = os.path.join(gam_dir, f\"{species}_GAM_Test_Metrics.txt\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, \"r\") as f:\n",
    "            metrics[\"GAM\"][species] = f.read()\n",
    "\n",
    "# =========================== 3️⃣ Load RF Models, Metrics & Predictions ===========================\n",
    "rf_dir = os.path.join(base_dir, \"RandomForest\")\n",
    "\n",
    "for species in species_list:\n",
    "    test_predictions[\"RF\"][species] = {}  # ✅ Initialise dictionary\n",
    "\n",
    "    species_dir = os.path.join(rf_dir, species)\n",
    "\n",
    "    # Load RF model\n",
    "    model_path = os.path.join(species_dir, \"RandomForest_Model.pkl\")\n",
    "    if os.path.exists(model_path):\n",
    "        models[\"RF\"][species] = joblib.load(model_path)\n",
    "\n",
    "    # Load RF test predictions\n",
    "    pred_path = os.path.join(species_dir, \"Test_Predictions.csv\")\n",
    "    if os.path.exists(pred_path):\n",
    "        test_predictions[\"RF\"][species] = pd.read_csv(pred_path)\n",
    "\n",
    "    # Load RF metrics\n",
    "    metrics_path = os.path.join(species_dir, \"RandomForest_Metrics.txt\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, \"r\") as f:\n",
    "            metrics[\"RF\"][species] = f.read()\n",
    "\n",
    "# =========================== 4️⃣ Load XGBoost Models, Metrics & Predictions ===========================\n",
    "xgb_dir = os.path.join(base_dir, \"XGBoost\")\n",
    "\n",
    "for species in species_list:\n",
    "    test_predictions[\"XGBoost\"][species] = {}  # ✅ Initialise dictionary\n",
    "\n",
    "    species_dir = os.path.join(xgb_dir, species)\n",
    "\n",
    "    # Load XGBoost model\n",
    "    model_path = os.path.join(species_dir, \"XGBoost_Model.pkl\")\n",
    "    if os.path.exists(model_path):\n",
    "        models[\"XGBoost\"][species] = joblib.load(model_path)\n",
    "\n",
    "    # Load XGBoost test predictions\n",
    "    pred_path = os.path.join(species_dir, \"Aggregated_Test_Predictions.csv\")\n",
    "    if os.path.exists(pred_path):\n",
    "        test_predictions[\"XGBoost\"][species] = pd.read_csv(pred_path)\n",
    "\n",
    "    # Load XGBoost metrics\n",
    "    metrics_path = os.path.join(species_dir, \"XGBoost_Metrics.txt\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, \"r\") as f:\n",
    "            metrics[\"XGBoost\"][species] = f.read()\n",
    "\n",
    "# =========================== 5️⃣ Load MaxEnt Test Predictions & Metrics ===========================\n",
    "maxent_dir = os.path.join(base_dir, \"Maxent\")\n",
    "\n",
    "for species in species_list:\n",
    "    test_predictions[\"MaxEnt\"][species] = {}  # ✅ Initialise dictionary\n",
    "\n",
    "    # Load MaxEnt test predictions\n",
    "    pred_path = os.path.join(maxent_dir, f\"Maxent_{species.replace(' ', '_')}_TestPredictions.csv\")\n",
    "    if os.path.exists(pred_path):\n",
    "        test_predictions[\"MaxEnt\"][species] = pd.read_csv(pred_path)\n",
    "\n",
    "# Load MaxEnt model evaluation metrics\n",
    "metrics_path = os.path.join(maxent_dir, \"Maxent_Model_Evaluation.csv\")\n",
    "if os.path.exists(metrics_path):\n",
    "    metrics[\"MaxEnt\"] = pd.read_csv(metrics_path)\n",
    "\n",
    "# =========================== ✅ Summary ===========================\n",
    "print(\"\\n✅ All models, test predictions, and metrics loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500946b4-df9c-46c0-be0f-0f2ae864ac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variable mapping file loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Variable Mapping File (if required)\n",
    "var_mapping_path = os.path.join(base_dir, \"Variable_Mapping.csv\")\n",
    "\n",
    "if os.path.exists(var_mapping_path):\n",
    "    variable_mapping = pd.read_csv(var_mapping_path)\n",
    "    print(\"✅ Variable mapping file loaded successfully!\")\n",
    "else:\n",
    "    variable_mapping = None\n",
    "    print(\"⚠️ Warning: Variable mapping file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f4eefd-2889-4033-834f-d0baf8945002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 GLM - Bufo bufo: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Bufo bufo_GLM_Lasso_TestPredictions.csv\n",
      "🔍 GLM - Bufo bufo: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Bufo bufo_GLM_Ridge_TestPredictions.csv\n",
      "🔍 GLM - Rana temporaria: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Rana temporaria_GLM_Lasso_TestPredictions.csv\n",
      "🔍 GLM - Rana temporaria: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Rana temporaria_GLM_Ridge_TestPredictions.csv\n",
      "🔍 GLM - Lissotriton helveticus: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Lissotriton helveticus_GLM_Lasso_TestPredictions.csv\n",
      "🔍 GLM - Lissotriton helveticus: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GLM\\GLM_Predictions\\Lissotriton helveticus_GLM_Ridge_TestPredictions.csv\n",
      "🔍 GAM - Bufo bufo: Columns Found -> ['label', 'prediction']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GAM\\GAM_Predictions\\Bufo_bufo_GAM_TestPredictions.csv\n",
      "🔍 GAM - Rana temporaria: Columns Found -> ['label', 'prediction']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GAM\\GAM_Predictions\\Rana_temporaria_GAM_TestPredictions.csv\n",
      "🔍 GAM - Lissotriton helveticus: Columns Found -> ['label', 'prediction']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Final_GAM\\GAM_Predictions\\Lissotriton_helveticus_GAM_TestPredictions.csv\n",
      "🔍 RF - Bufo bufo: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\RandomForest\\Bufo_bufo\\Test_Predictions.csv\n",
      "🔍 RF - Rana temporaria: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\RandomForest\\Rana_temporaria\\Test_Predictions.csv\n",
      "🔍 RF - Lissotriton helveticus: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\RandomForest\\Lissotriton_helveticus\\Test_Predictions.csv\n",
      "🔍 XGBoost - Bufo bufo: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\XGBoost\\Bufo_bufo\\Aggregated_Test_Predictions.csv\n",
      "🔍 XGBoost - Rana temporaria: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\XGBoost\\Rana_temporaria\\Aggregated_Test_Predictions.csv\n",
      "🔍 XGBoost - Lissotriton helveticus: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\XGBoost\\Lissotriton_helveticus\\Aggregated_Test_Predictions.csv\n",
      "🔍 MaxEnt - Bufo bufo: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Maxent\\Maxent_Bufo_bufo_TestPredictions.csv\n",
      "🔍 MaxEnt - Rana temporaria: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Maxent\\Maxent_Rana_temporaria_TestPredictions.csv\n",
      "🔍 MaxEnt - Lissotriton helveticus: Columns Found -> ['True_Label', 'Predicted_Probability']\n",
      "✅ Standardised and saved: C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\\Maxent\\Maxent_Lissotriton_helveticus_TestPredictions.csv\n",
      "\n",
      "✅ Consistency check complete!\n",
      "\n",
      "\n",
      "🚀 Ready to proceed once inconsistencies are resolved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base directories for all models\n",
    "base_dir = r\"C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\"\n",
    "\n",
    "# Define species list\n",
    "species_list = [\"Bufo bufo\", \"Rana temporaria\", \"Lissotriton helveticus\"]\n",
    "\n",
    "# Define expected columns\n",
    "expected_columns = [\"True_Label\", \"Predicted_Probability\"]\n",
    "\n",
    "# Define paths for each model\n",
    "model_dirs = {\n",
    "    \"GLM\": os.path.join(base_dir, \"Final_GLM\", \"GLM_Predictions\"),\n",
    "    \"GAM\": os.path.join(base_dir, \"Final_GAM\", \"GAM_Predictions\"),\n",
    "    \"RF\": os.path.join(base_dir, \"RandomForest\"),\n",
    "    \"XGBoost\": os.path.join(base_dir, \"XGBoost\"),\n",
    "    \"MaxEnt\": os.path.join(base_dir, \"Maxent\")\n",
    "}\n",
    "\n",
    "# Define filenames (model-specific variations)\n",
    "file_patterns = {\n",
    "    \"GLM\": \"{species}_GLM_{glm_type}_TestPredictions.csv\",\n",
    "    \"GAM\": \"{species}_GAM_TestPredictions.csv\",\n",
    "    \"RF\": os.path.join(\"{species}\", \"Test_Predictions.csv\"),\n",
    "    \"XGBoost\": os.path.join(\"{species}\", \"Aggregated_Test_Predictions.csv\"),\n",
    "    \"MaxEnt\": \"Maxent_{species}_TestPredictions.csv\"\n",
    "}\n",
    "\n",
    "# GLM-specific model types\n",
    "glm_types = [\"Lasso\", \"Ridge\"]\n",
    "\n",
    "# Dictionary to store results\n",
    "inconsistencies = {}\n",
    "\n",
    "# Function to check file existence, column names, and format\n",
    "def check_predictions():\n",
    "    global inconsistencies\n",
    "    inconsistencies = {}  # Reset inconsistencies\n",
    "    \n",
    "    for model, model_dir in model_dirs.items():\n",
    "        inconsistencies[model] = {}\n",
    "        \n",
    "        for species in species_list:\n",
    "            formatted_species = species.replace(\" \", \"_\")  # Adjust species naming for files\n",
    "            \n",
    "            if model == \"GLM\":\n",
    "                for glm_type in glm_types:\n",
    "                    file_path = os.path.join(model_dir, file_patterns[model].format(species=species, glm_type=glm_type))\n",
    "                    check_file(file_path, model, species, glm_type)\n",
    "            else:\n",
    "                file_path = os.path.join(model_dir, file_patterns[model].format(species=formatted_species))\n",
    "                check_file(file_path, model, species)\n",
    "    \n",
    "    print(\"\\n✅ Consistency check complete!\\n\")\n",
    "    return inconsistencies\n",
    "\n",
    "# Function to check individual files\n",
    "def check_file(file_path, model, species, glm_type=None):\n",
    "    if model not in inconsistencies:\n",
    "        inconsistencies[model] = {}\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        inconsistencies[model][species] = f\"❌ Missing file: {file_path}\"\n",
    "        return\n",
    "    \n",
    "    # Load the file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        inconsistencies[model][species] = f\"❌ Error loading file: {str(e)}\"\n",
    "        return\n",
    "\n",
    "    # Print actual column names for debugging\n",
    "    print(f\"🔍 {model} - {species}: Columns Found -> {df.columns.tolist()}\")\n",
    "\n",
    "    # Automatically detect the correct column names\n",
    "    true_label_col = None\n",
    "    predicted_prob_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if \"true\" in col.lower() or \"label\" in col.lower():\n",
    "            true_label_col = col\n",
    "        if \"prob\" in col.lower() or \"prediction\" in col.lower():\n",
    "            predicted_prob_col = col\n",
    "\n",
    "    # Rename columns if detected correctly\n",
    "    if true_label_col and predicted_prob_col:\n",
    "        df = df.rename(columns={true_label_col: \"True_Label\", predicted_prob_col: \"Predicted_Probability\"})\n",
    "    else:\n",
    "        inconsistencies[model][species] = f\"⚠️ Column mismatch: Found {df.columns.tolist()}, expected {expected_columns}\"\n",
    "        return\n",
    "\n",
    "    # Check for missing values\n",
    "    if df.isnull().any().sum() > 0:\n",
    "        inconsistencies[model][species] = \"⚠️ Missing values found in test predictions\"\n",
    "\n",
    "    # Check data types\n",
    "    if not df[\"True_Label\"].dtype == \"int64\":\n",
    "        inconsistencies[model][species] = \"⚠️ 'True_Label' column should be int64\"\n",
    "    if not pd.api.types.is_numeric_dtype(df[\"Predicted_Probability\"]):\n",
    "        inconsistencies[model][species] = \"⚠️ 'Predicted_Probability' column should be numeric\"\n",
    "\n",
    "    # Save back the corrected version\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"✅ Standardised and saved: {file_path}\")\n",
    "\n",
    "# Run the consistency check\n",
    "inconsistencies = check_predictions()\n",
    "\n",
    "# Print inconsistencies found\n",
    "for model, issues in inconsistencies.items():\n",
    "    if issues:\n",
    "        print(f\"\\n🔍 Inconsistencies in {model}:\")\n",
    "        for species, issue in issues.items():\n",
    "            print(f\"  - {species}: {issue}\")\n",
    "\n",
    "print(\"\\n🚀 Ready to proceed once inconsistencies are resolved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb2005-6db7-4ae7-95e5-0434965cdef9",
   "metadata": {},
   "source": [
    "## 3. Model Performance Evaluation\n",
    "\n",
    "### **Metrics used in Evaluation**:\n",
    "\n",
    "- **Area Under the Receiver Operating Characteristic Curve (AUC-ROC)**: Measures the ability of the model to distinguish between classes.\n",
    "- **Confusion Matrix:** Provides a summary of prediction results, showing true positives, true negatives, false positives, and false negatives.\n",
    "- **Precision, Recall, and F1-Score:** Evaluate the accuracy of positive predictions, the ability to find all positive instances, and the balance between precision and recall, respectively.\n",
    "\n",
    "### **Rationale:**\n",
    "These metrics offer a comprehensive view of each model's performance, highlighting different aspects of predictive accuracy and error rates.\n",
    "\n",
    "### 3.1 Load all test predictions into a structured format\n",
    "- Combine results into a single pandas DataFrame for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38e5ee26-5e83-404c-b811-d11e1349d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c354053-2540-426e-8974-f9e7f792cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded test predictions for Bufo bufo - GLM_Lasso\n",
      "✅ Loaded test predictions for Rana temporaria - GLM_Lasso\n",
      "✅ Loaded test predictions for Lissotriton helveticus - GLM_Lasso\n",
      "✅ Loaded test predictions for Bufo bufo - GLM_Ridge\n",
      "✅ Loaded test predictions for Rana temporaria - GLM_Ridge\n",
      "✅ Loaded test predictions for Lissotriton helveticus - GLM_Ridge\n",
      "✅ Loaded test predictions for Bufo bufo - GAM\n",
      "✅ Loaded test predictions for Rana temporaria - GAM\n",
      "✅ Loaded test predictions for Lissotriton helveticus - GAM\n",
      "✅ Loaded test predictions for Bufo bufo - RF\n",
      "✅ Loaded test predictions for Rana temporaria - RF\n",
      "✅ Loaded test predictions for Lissotriton helveticus - RF\n",
      "✅ Loaded test predictions for Bufo bufo - XGBoost\n",
      "✅ Loaded test predictions for Rana temporaria - XGBoost\n",
      "✅ Loaded test predictions for Lissotriton helveticus - XGBoost\n",
      "✅ Loaded test predictions for Bufo bufo - MaxEnt\n",
      "✅ Loaded test predictions for Rana temporaria - MaxEnt\n",
      "✅ Loaded test predictions for Lissotriton helveticus - MaxEnt\n"
     ]
    }
   ],
   "source": [
    "# Base directory containing the test predictions\n",
    "base_dir = r\"C:\\GIS_Course\\MScThesis-MaviSantarelli\\results\\Models\"\n",
    "\n",
    "# Define species list\n",
    "species_list = [\"Bufo bufo\", \"Rana temporaria\", \"Lissotriton helveticus\"]\n",
    "\n",
    "# Define model names\n",
    "models = [\"GLM_Lasso\", \"GLM_Ridge\", \"GAM\", \"RF\", \"XGBoost\", \"MaxEnt\"]\n",
    "\n",
    "# Dictionary to store test predictions\n",
    "test_predictions = {model: {} for model in models}\n",
    "\n",
    "# File paths for each model\n",
    "file_paths = {\n",
    "    \"GLM_Lasso\": os.path.join(base_dir, \"Final_GLM\", \"GLM_Predictions\", \"{species}_GLM_Lasso_TestPredictions.csv\"),\n",
    "    \"GLM_Ridge\": os.path.join(base_dir, \"Final_GLM\", \"GLM_Predictions\", \"{species}_GLM_Ridge_TestPredictions.csv\"),\n",
    "    \"GAM\": os.path.join(base_dir, \"Final_GAM\", \"GAM_Predictions\", \"{species}_GAM_TestPredictions.csv\"),\n",
    "    \"RF\": os.path.join(base_dir, \"RandomForest\", \"{species}\", \"Test_Predictions.csv\"),\n",
    "    \"XGBoost\": os.path.join(base_dir, \"XGBoost\", \"{species}\", \"Aggregated_Test_Predictions.csv\"),\n",
    "    \"MaxEnt\": os.path.join(base_dir, \"Maxent\", \"Maxent_{species}_TestPredictions.csv\")\n",
    "}\n",
    "\n",
    "# Standard column names for consistency\n",
    "expected_columns = [\"True_Label\", \"Predicted_Probability\"]\n",
    "\n",
    "# Load each model's test predictions\n",
    "for model, path_template in file_paths.items():\n",
    "    for species in species_list:\n",
    "        formatted_species = species.replace(\" \", \"_\")  # Ensure file names match correctly\n",
    "        file_path = path_template.format(species=formatted_species)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardise column names if needed\n",
    "            df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "            if \"label\" in df.columns:\n",
    "                df.rename(columns={\"label\": \"true_label\"}, inplace=True)\n",
    "            if \"prediction\" in df.columns:\n",
    "                df.rename(columns={\"prediction\": \"predicted_probability\"}, inplace=True)\n",
    "            if \"avg_prediction\" in df.columns:\n",
    "                df.rename(columns={\"avg_prediction\": \"predicted_probability\"}, inplace=True)\n",
    "\n",
    "            # Store the standardised dataframe\n",
    "            test_predictions[model][species] = df\n",
    "            print(f\"✅ Loaded test predictions for {species} - {model}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Missing test predictions for {species} - {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557933f1-a50b-4ed0-8481-a36630b0b2e0",
   "metadata": {},
   "source": [
    "### 3.2 Compute Performance Metrics\n",
    "- Calculate AUC-ROC, Precision, Recall, and F1-score for each model and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83116dfb-bba4-4054-9416-b8874f1de934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Performance metrics calculated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store performance metrics\n",
    "performance_metrics = {species: {} for species in species_list}\n",
    "\n",
    "# Compute metrics for each species and model\n",
    "for species in species_list:\n",
    "    performance_metrics[species] = {}\n",
    "\n",
    "    for model in models:\n",
    "        if species in test_predictions[model]:  # Ensure predictions exist\n",
    "            df = test_predictions[model][species]\n",
    "            \n",
    "            # Extract true labels and predicted probabilities\n",
    "            y_true = df[\"true_label\"]\n",
    "            y_pred_prob = df[\"predicted_probability\"]\n",
    "            y_pred = (y_pred_prob >= 0.5).astype(int)  # Convert probability to binary predictions\n",
    "\n",
    "            # Compute performance metrics\n",
    "            auc_roc = roc_auc_score(y_true, y_pred_prob)\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "            # Store results\n",
    "            performance_metrics[species][model] = {\n",
    "                \"AUC-ROC\": round(auc_roc, 3),\n",
    "                \"Precision\": round(precision, 3),\n",
    "                \"Recall\": round(recall, 3),\n",
    "                \"F1 Score\": round(f1, 3)\n",
    "            }\n",
    "        else:\n",
    "            print(f\"⚠️ Skipping {species} - {model} (Missing test predictions)\")\n",
    "\n",
    "print(\"\\n✅ Performance metrics calculated successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c344f86-1d28-4e6d-b772-5e1b718bec55",
   "metadata": {},
   "source": [
    "### 3.3 Create a Summary Table\n",
    "- Display model performances side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a28d83c-d324-4d21-b6c8-23c9b506b2da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m summary_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Display summary\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance Summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39msummary_df)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Convert the nested dictionary into a pandas DataFrame\n",
    "summary_df = pd.DataFrame.from_dict(\n",
    "    {(species, model): metrics for species, models in performance_metrics.items() for model, metrics in models.items()},\n",
    "    orient=\"index\"\n",
    ")\n",
    "\n",
    "# Reset index for better formatting\n",
    "summary_df.index = pd.MultiIndex.from_tuples(summary_df.index, names=[\"Species\", \"Model\"])\n",
    "summary_df.reset_index(inplace=True)\n",
    "\n",
    "# Display summary\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Model Performance Summary\", dataframe=summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fa367-10f2-4a39-8017-b20e0a90b7a5",
   "metadata": {},
   "source": [
    "## 4. Visualisation of Model Performance\n",
    "\n",
    "### **Steps:**\n",
    "#### 1. Plot ROC Curves:\n",
    "- Visualise the trade-off between true positive and false positive rates for each model.\n",
    "\n",
    "#### 2. Compare Performance Metrics:\n",
    "- Create bar plots or tables to compare precision, recall, and F1-scores across models.\n",
    "\n",
    "### **Rationale:**\n",
    "Visual representations facilitate intuitive comparisons and help identify models that perform well across various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f7b37-8f8d-4fc9-8c24-7d4484ff7f5c",
   "metadata": {},
   "source": [
    "## 5. Ensemble Modelling\n",
    "\n",
    "### **Techniques to Explore:**\n",
    "\n",
    "- **Weighted Averaging:** Combine model predictions by assigning weights proportional to their performance metrics.\n",
    "- **Majority Voting:** For classification tasks, predict the class that receives the majority vote from individual models.\n",
    "\n",
    "### **Rationale:**\n",
    "Ensemble methods aim to harness the strengths of multiple models, often resulting in improved predictive performance and reduced variance (Meller et al., 2014). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697bb4d-97b0-4950-bf63-555a6d6a25b3",
   "metadata": {},
   "source": [
    "## 6. Evaluation of Ensemble Models\n",
    "\n",
    "### **Steps:**\n",
    "- **Compute Performance Metrics:** Assess the ensemble models using the same metrics as individual models (AUC-ROC, precision, recall, F1-score).\n",
    "- **Compare to Individual Models:** Determine if the ensemble models outperform individual models.\n",
    "\n",
    "### **Rationale:**\n",
    "Evaluating ensemble models against individual models helps in understanding the added value of ensembling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399706d-5476-43f6-9908-2dd9035ccfa2",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00b7bc-2116-4412-ac17-bb14b144d247",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Meller, L., Cabeza, M., Pironon, S., Barbet-Massin, M., Maiorano, L., Georges, D., & Thuiller, W. (2014). Ensemble distribution models in conservation prioritization: From consensus predictions to consensus reserve networks. *Diversity and Distributions*, 20(3), 309–321. https://doi.org/10.1111/ddi.12162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf806f-9f4e-403d-b7d2-acef2ff80ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MscThesis)",
   "language": "python",
   "name": "mscthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
